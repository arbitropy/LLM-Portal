import os
from typing import Iterator
import gradio as gr
from dotenv import load_dotenv
from distutils.util import strtobool
from inference_scripts.model import INFERENCE
import logging
import torchaudio
import langid
from seamless_communication.inference import Translator
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import torch 


# loading facebookm4t model full, look into chunked model
AUDIO_SAMPLE_RATE = 16000.0
MAX_INPUT_AUDIO_LENGTH = 60 

if torch.cuda.is_available():
    device = torch.device("cuda:0")
    dtype = torch.float16
else:
    device = torch.device("cpu")
    dtype = torch.float32

# # using pipeline doesn't work with gradio, below is from facebooks demo
# translator = Translator(
#     model_name_or_card="seamlessM4T_v2_large",
#     vocoder_name_or_card="vocoder_v2",
#     device=device,
#     dtype=dtype,
#     apply_mintox=True,
# )

# def preprocess_audio(input_audio: str) -> None:
#     if type(input_audio) is not str:
#         return input_audio
#     arr, org_sr = torchaudio.load(input_audio)
#     new_arr = torchaudio.functional.resample(arr, orig_freq=org_sr, new_freq=AUDIO_SAMPLE_RATE)
#     max_length = int(MAX_INPUT_AUDIO_LENGTH * AUDIO_SAMPLE_RATE)
#     if new_arr.shape[1] > max_length:
#         new_arr = new_arr[:, :max_length]
#         gr.Warning(f"Input audio is too long. Only the first {MAX_INPUT_AUDIO_LENGTH} seconds is used.")
#     torchaudio.save(input_audio, new_arr, sample_rate=int(AUDIO_SAMPLE_RATE))
# # translate 
# def run_t2tt(input_text: str, source_language: str, target_language: str) -> str:
#     out_texts, _ = translator.predict(
#         input=input_text,
#         task_str="T2TT",
#         src_lang=source_language,
#         tgt_lang=target_language,
#     )
#     return str(out_texts[0])

# def translate(message):
#     langid.set_languages(['ar', 'en'])
#     lang, score = langid.classify(message)
#     if (lang == 'en'):
#         return run_t2tt(message, "eng", "arb")
#     elif (lang == 'ar'):
#         return run_t2tt(message, "arb", "eng")
#     else:
#         return message

# def run_asr(input_audio: str) -> str:
#     if input_audio is None:
#         return ""
#     preprocess_audio(input_audio)
#     target_language_code = "eng"
#     out_texts, _ = translator.predict(
#         input=input_audio,
#         task_str="ASR",
#         src_lang=target_language_code,
#         tgt_lang=target_language_code,
#     )
#     return str(out_texts[0])

# whisper ASR setup
device = "cuda:0" if torch.cuda.is_available() else "cpu"
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

import whisper

whisperModel = whisper.load_model("base")
def run_asr(input_audio: str) -> str:
    if input_audio is None:
        return ""
    return whisperModel.transcribe(input_audio)["text"]

# facebook mmt translation setup
# mmtModel = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
# mmtTokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")

mmtModel = AutoModelForSeq2SeqLM.from_pretrained("google-t5/t5-small")
mmtTokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")

def run_t2tt(input_text: str, source_language: str, target_language: str) -> str:
    mmtTokenizer.src_lang = source_language
    encoded_ar = mmtTokenizer(input_text, return_tensors="pt")
    generated_tokens = mmtModel.generate(
    **encoded_ar,
    forced_bos_token_id=mmtTokenizer.lang_code_to_id[target_language]
    )
    return mmtTokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]

def translate(message):
    langid.set_languages(['ar', 'en'])
    lang, score = langid.classify(message)
    if (lang == 'en'):
        return run_t2tt(message, "en_XX", "ar_AR")
    elif (lang == 'ar'):
        return run_t2tt(message, "ar_AR", "en_XX")
    else:
        return message

load_dotenv()


DEFAULT_SYSTEM_PROMPT = os.getenv("DEFAULT_SYSTEM_PROMPT", "")
MAX_MAX_NEW_TOKENS = int(os.getenv("MAX_MAX_NEW_TOKENS", 2048))
DEFAULT_MAX_NEW_TOKENS = int(os.getenv("DEFAULT_MAX_NEW_TOKENS", 1024))
MAX_INPUT_TOKEN_LENGTH = int(os.getenv("MAX_INPUT_TOKEN_LENGTH", 4000))

MODEL_PATH = os.getenv("MODEL_PATH")
assert MODEL_PATH is not None, f"MODEL_PATH is required, got: {MODEL_PATH}"
BACKEND_TYPE = "transformers"

LOAD_IN_8BIT = bool(strtobool(os.getenv("LOAD_IN_8BIT", "True")))

model_instance = INFERENCE(
    model_path=MODEL_PATH,
    backend_type=BACKEND_TYPE,
    max_tokens=MAX_INPUT_TOKEN_LENGTH,
    load_in_8bit=LOAD_IN_8BIT,
    # verbose=True,
)

DESCRIPTION = """
# **HajjLLM**
Disclaimer: This model is experimental and may produce wrong or hallucinated responses. Do not take them as religious rulings or authoritative guidance. This system is in a testing phase and is continually being improved.
"""

def clear_and_save_textbox(message: str) -> tuple[str, str]:
    return "", message

def check_empty_input(message: str):
    if len(message) == 0:
        raise gr.Error(
            f"Please provide a valid non empty input."
        )

def check_empty_audio(audio):
    if audio is None:
        raise gr.Error(
            f"Please provide a non empty audio."
        )

def display_input(
    message: str, history: list[tuple[str, str]]
) -> list[tuple[str, str]]:
    
    history.append((message, ""))
    return history

def delete_prev_fn(
    history: list[tuple[str, str]]
) -> tuple[list[tuple[str, str]], str]:
    try:
        message, _ = history.pop()
    except IndexError:
        message = ""
    return history, message or ""


def generate(
    message: str,
    history_with_input: list[tuple[str, str]],
    system_prompt: str,
    max_new_tokens: int,
    temperature: float,
    top_p: float,
    top_k: int,
) -> Iterator[list[tuple[str, str]]]:
    if max_new_tokens > MAX_MAX_NEW_TOKENS:
        raise ValueError
    try:
        langid.set_languages(['ar', 'en'])
        lang, score = langid.classify(message)
        english = (lang == 'en')
        # print(lang)
        # print(history_with_input)
        history = history_with_input[:-1]
        # tranlates message if not in english
        if (not english):
            # print(message)
            processedMessage = translate(message) # translate for generation if not english
            # print(processedMessage)
        else:
            # print(message)
            processedMessage = message
            
        generator = model_instance.run(
            processedMessage,
            history,
            system_prompt,
            max_new_tokens,
            temperature,
            top_p,
            top_k,
        )
        
        try:
            first_response = next(generator)
            yield history + [(message, first_response)]
        except StopIteration:
            yield history + [(message, "")]
        for response in generator:
            yield history + [(message, response)]
        # translate after the entire chat is generated
        # if(not english):
        #     print(response)
        #     response = translate(response) 
        #     print(response)
        #     yield history + [(message, response)]
        # print("response: "+response)
    except Exception as e:
        logging.exception(e)

def check_input_token_length(
    message: str, chat_history: list[tuple[str, str]], system_prompt: str
) -> None:
    # print(message)
    input_token_length = model_instance.get_input_token_length(
        message, chat_history, system_prompt
    )
    if input_token_length > MAX_INPUT_TOKEN_LENGTH:
        raise gr.Error(
            f"The accumulated input is too long ({input_token_length} > {MAX_INPUT_TOKEN_LENGTH}). Clear your chat history and try again."
        )

with open("custom.css", "r", encoding="utf-8") as f:
    CSS = f.read()

with gr.Blocks(css=CSS, theme=gr.themes.Default(text_size = "sm")) as demo:
    state = gr.State(value="")
    with gr.Row():
        gr.Markdown(DESCRIPTION)
    with gr.Row(equal_height=True):
        with gr.Column(scale=3):
            with gr.Group():
                chatbot = gr.Chatbot(label="Chatbot")
                with gr.Row():
                    textbox = gr.Textbox(
                        container=False,
                        show_label=False,
                        placeholder="Type a message...",
                        scale=20,
                    )
                    submit_button = gr.Button(
                        "Submit", variant="primary", scale=1, min_width=0
                    )
        with gr.Column():
            with gr.Row():
                with gr.Group():
                    with gr.Column():
                        audio2 = gr.Audio(sources = ["microphone"], type="filepath", interactive = True) 
                        audio_submit = gr.Button("Submit") 
            with gr.Row():
                upload_checkbox = gr.Checkbox(
                    label="Upload",
                    value=False,
                    container=False,
                    elem_classes="min_check",
                )
            with gr.Row(visible = False) as upload_row:
                audio = gr.Audio(sources = ["upload"], type="filepath")
                
            with gr.Row():
                retry_button = gr.Button("üîÑ  Retry", variant="secondary")
                undo_button = gr.Button("‚Ü©Ô∏è Undo", variant="secondary")
                clear_button = gr.Button("üóëÔ∏è  Clear", variant="secondary")

            saved_input = gr.State()
            with gr.Row():
                advanced_checkbox = gr.Checkbox(
                    label="Advanced",
                    value=False,
                    container=False,
                    elem_classes="min_check",
                )
            with gr.Column(visible=False) as advanced_column:
                system_prompt = gr.Textbox(
                    label="System prompt", value=DEFAULT_SYSTEM_PROMPT, lines=6
                )
                max_new_tokens = gr.Slider(
                    label="Max new tokens",
                    minimum=1,
                    maximum=MAX_MAX_NEW_TOKENS,
                    step=1,
                    value=DEFAULT_MAX_NEW_TOKENS,
                )
                temperature = gr.Slider(
                    label="Temperature",
                    minimum=0.01,
                    maximum=4.0,
                    step=0.1,
                    value=0.0,
                )
                top_p = gr.Slider(
                    label="Top-p (nucleus sampling)",
                    minimum=0.05,
                    maximum=1.0,
                    step=0.05,
                    value=0.95,
                )
                top_k = gr.Slider(
                    label="Top-k",
                    minimum=1,
                    maximum=1000,
                    step=1,
                    value=50,
                )
        advanced_checkbox.change(
            lambda x: gr.update(visible=x),
            advanced_checkbox,
            advanced_column,
            queue=False, 
        )
        upload_checkbox.change(
            lambda x: gr.update(visible=x),
            upload_checkbox,
            upload_row,
            queue=False,
        ) 

    textbox.submit(
        fn=check_empty_input,
        inputs=textbox,
        api_name=False,
        queue=False,
    ).success(
        fn=clear_and_save_textbox,
        inputs=textbox,
        outputs=[textbox, saved_input],
        api_name=False,
        queue=False,
    ).then(
        fn=display_input,
        inputs=[saved_input, chatbot],
        outputs=chatbot,
        api_name=False,
        queue=False,
    ).then(
        fn=check_input_token_length,
        inputs=[saved_input, chatbot, system_prompt],
        api_name=False,
        queue=False,
    ).success(
        fn=generate,
        inputs=[
            saved_input,
            chatbot,
            system_prompt,
            max_new_tokens,
            temperature,
            top_p,
            top_k,
        ],
        outputs=chatbot,
        api_name=False,
    )

    audio_submit.click(
        fn=check_empty_audio,
        inputs=audio2,
        api_name=False,
        queue=False,
    ).success(
        fn=run_asr,
        inputs=audio2,
        outputs=textbox,
        api_name="asr",
    )
    audio.upload(
        fn=run_asr,
        inputs=audio,
        outputs=textbox,
        api_name="ast",
    )

    button_event_preprocess = (
        submit_button.click(
        fn=check_empty_input,
        inputs=textbox,
        api_name=False,
        queue=False,
        ).success(
            fn=clear_and_save_textbox,
            inputs=textbox,
            outputs=[textbox, saved_input],
            api_name=False,
            queue=False,
        )
        .then(
            fn=display_input,
            inputs=[saved_input, chatbot],
            outputs=chatbot,
            api_name=False,
            queue=False,
        )
        .then(
            fn=check_input_token_length,
            inputs=[saved_input, chatbot, system_prompt],
            api_name=False,
            queue=False,
        )
        .success(
            fn=generate,
            inputs=[
                saved_input,
                chatbot,
                system_prompt,
                max_new_tokens,
                temperature,
                top_p,
                top_k,
            ],
            outputs=chatbot,
            api_name=False,
        )
    )

    retry_button.click(
        fn=delete_prev_fn,
        inputs=chatbot,
        outputs=[chatbot, saved_input],
        api_name=False,
        queue=False,
    ).then(
        fn=display_input,
        inputs=[saved_input, chatbot],
        outputs=chatbot,
        api_name=False,
        queue=False,
    ).then(
        fn=generate,
        inputs=[
            saved_input,
            chatbot,
            system_prompt,
            max_new_tokens,
            temperature,
            top_p,
            top_k,
        ],
        outputs=chatbot,
        api_name=False,
    )

    undo_button.click(
        fn=delete_prev_fn,
        inputs=chatbot,
        outputs=[chatbot, saved_input],
        api_name=False,
        queue=False,
    ).then(
        fn=lambda x: x,
        inputs=[saved_input],
        outputs=textbox,
        api_name=False,
        queue=False,
    )

    clear_button.click(
        fn=lambda: ([], ""),
        outputs=[chatbot, saved_input],
        queue=False,
        api_name=False,
    )


if __name__ == "__main__":
    demo.queue()
    demo.launch()
